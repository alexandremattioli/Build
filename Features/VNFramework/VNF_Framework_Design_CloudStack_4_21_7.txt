VNF Framework Feature Design and Implementation (CloudStack 4.21.7)
Overview
Apache CloudStack 4.19 introduced the concept of **VNF Templates** and **VNF Appliances** as an alternative to the built-in Virtual Router (VR) for network services?9†L130-L138?. A VNF (Virtual Network Function) appliance is essentially a user-deployed virtual router, firewall, or similar network device, which can serve as the gateway and provide services (firewall, NAT, load balancing, etc.) for a guest network. The **VNF Framework** in CloudStack 4.21.7 aims to fully integrate these third-party network appliances into CloudStack’s control plane. This means CloudStack will not only deploy the VNF VMs, but also manage their network configuration through standard CloudStack APIs and UI, just like it does for the native VR. The goal is to allow tenants to create VNF-backed networks where their chosen VNF VM is the data-plane gateway, while CloudStack provides a unified management interface across different vendors?26†L179-L183?. Key aspects of this feature include new control-plane APIs, service-layer logic to orchestrate calls to the VNF, use of the existing CloudStack VR as a **broker** (intermediary) for communication, a **YAML-based dictionary** that maps CloudStack’s generic network commands to vendor-specific API calls, a secure communication channel (mTLS + JWT) for sending instructions, integration with CloudStack’s secret management for sensitive credentials, and UI enhancements in the Primate console. The design emphasizes a clean separation of responsibilities and alignment with CloudStack’s pluggable service-provider model – meaning the VNF integration will be built as a modular provider that fits into CloudStack’s networking framework?26†L211-L218?.
Architecture and Components
?27†embed_image? *Figure: High-level architecture of the VNF Framework. The CloudStack management server (left) contains VNF service modules (dictionary parser, request renderer, etc.) and leverages connectors (SSH, HTTP) and data format handlers (JSON, XML, text) to communicate with VNF devices. A Virtual Router acts as a **VNF broker** (middle), forwarding CloudStack’s control-plane requests over a secure channel to the VNF appliance (right). This modular design allows CloudStack to support multiple vendors and protocols by simply swapping out dictionary definitions or connectors.*
**Management Server Components:** On the CloudStack management side, a new **VNF Service** component will coordinate all VNF-related operations. This may be implemented as part of the NetworkManager or as a separate manager module. It will house the logic for handling API calls related to VNFs and networks, and it will interface with CloudStack’s existing networking subsystem. The design is modular: we introduce a **VNF Provider** (or a set of providers) that plug into CloudStack’s network service provider framework. Each provider corresponds to a way of managing a VNF (for now, a generic dictionary-driven provider via the VR broker). Internally, this provider will use a **Dictionary Parser and Renderer** to interpret the YAML definitions and construct the appropriate device-specific commands. The management server also includes or utilizes **Connector** modules for communication – e.g. an HTTP client for REST-based VNFs or an SSH client for CLI-based VNFs. These connectors handle the low-level protocol with the VNF (issuing HTTP requests, handling TLS, or opening SSH sessions). The outcome is that the management server can translate a high-level request (like “add firewall rule”) into a vendor-specific action and send it out using the correct protocol.
**Virtual Router as Broker:** The CloudStack Virtual Router remains present in VNF-backed networks, but its role changes to a **broker and helper**. Instead of being the primary gateway, the VR is deployed in a supportive capacity: it still provides DHCP, DNS, and metadata services to the guest network, but it no longer routes tenant traffic (that duty passes to the VNF VM). More importantly, the VR acts as a trusted intermediary for control-plane messages. The management server will communicate with the VNF through the VR – effectively using the VR as a proxy or message broker. This is necessary because in many CloudStack deployments, the management server may not have direct IP connectivity to the isolated guest network where the VNF resides. By leveraging the VR (which does have a foothold in that network), CloudStack can reach the VNF’s management interface without exposing it externally. We design this broker communication to be abstract, anticipating that in the future an **external broker** or controller could replace the VR’s role. (For example, an enterprise might have a centralized SDN controller that CloudStack could call, or CloudStack might talk directly to the VNF if networking allows?26†L237-L242?.) The system should be built such that switching the broker is possible with minimal changes – e.g., via a provider setting or configuration.
**VNF Appliance (Data-Plane):** The VNF itself is a VM running the vendor’s network OS or software appliance (for example, a virtual Palo Alto firewall, a VyOS router, a FortiGate appliance, or even an open-source pfSense). This VM has one or more NICs attached to CloudStack networks. In a typical isolated network scenario, the VNF will have at least one NIC on the guest network (to talk to the guest VMs) and potentially one NIC on a public or upstream network for internet connectivity. In CloudStack’s VNF-backed network, the **tenant VNF VM is the default gateway for the guest VMs**. That means CloudStack must ensure the guest VMs’ DHCP default route points to the VNF’s IP, not to the VR’s IP. (The VR can either have no gateway IP or a different IP on the subnet purely for DHCP/DNS.) The VNF appliance will receive configuration commands via its management interface (which could be one of the NICs, or a specific IP on the guest NIC reserved for management). It’s the device that actually applies firewall rules, NAT, VPN, etc., in the data plane, according to its vendor-specific software.
By structuring the system into these components – Management (with dictionary and connectors), Broker (VR), and Appliance (VNF) – we achieve a clean separation where CloudStack handles the *control plane* (API, logic, orchestration) and the VNF handles the *data plane* (actual packet processing), with the VR bridging the gap.
Control-Plane APIs
To support the VNF framework, several API additions and changes are required on the CloudStack management side. These APIs cover managing VNF templates and dictionaries, creating networks with VNF appliances, and performing new operations like reconciliation. Below is a summary of the key API considerations:
- **VNF Template Management:** CloudStack already allows registering a template as type "VNF" and updating its settings (NIC mappings, details for access)?19†L140-L148??19†L179-L186?. We will extend this with an API (and UI) to manage the **VNF dictionary** attached to a template. This could be a new API command, e.g., `updateTemplateDictionary`, which accepts a template ID and a YAML text blob (or a URL to fetch it from). Uploading a dictionary populates a stored text field or separate table in CloudStack’s database associated with that template. We should also provide a way to retrieve the dictionary (for editing or debugging), perhaps via `getTemplateDictionary` (admin-only, since it might contain sensitive structure). For safety, only users with proper rights (likely CloudStack admins or template owners) can upload or modify the dictionary on a template.
- **Network Creation with VNF:** We introduce the ability to create an **Isolated Network** that is backed by a VNF appliance. This could be done by enhancing the existing `createNetwork` API. New parameters might include:
  - `vnftemplateid`: the ID of the VNF Template to use as the network’s gateway device.
  - `vnfdictionary`: (optional) a YAML text or an ID of a previously uploaded dictionary to override the template’s default dictionary for this network.
  - Alternatively, the API might allow specifying a flag like `ispersistent=true` plus a network offering that implies a VNF, but it’s cleaner to have an explicit template reference.
  
  Under the hood, when CloudStack processes a `createNetwork` with a `vnftemplateid`, it knows this network will use a VNF provider. It will likely select a special **Network Offering** that includes the VNF provider for services (firewall, etc.) and the VR for DHCP/DNS. This can be automated by the management server or the API can require an appropriate offering to be passed. (An admin might pre-create a network offering e.g. "Isolated Network with VNF Gateway" that has the needed service-provider mappings.)
- **VNF Appliance Deployment (Automated):** In CloudStack 4.19/4.21, there was a separate API `deployVnfAppliance`?12†L6-L14? to manually launch a VNF VM on specified networks. For the integrated workflow, we want the network creation to automatically deploy the VNF appliance VM as the gateway. This likely means that after `createNetwork` allocates the network, an **async job** kicks off to deploy the VNF VM (using the given template and a suitable service offering for the VM’s compute resources). The API response for network creation may include the ID of the VNF appliance or its status as part of the network creation result.
- **Network Operations (Firewall, NAT, etc.):** One design goal is that from the **user’s perspective**, managing a VNF-backed network should be very similar to managing a normal CloudStack network. Users will use the same APIs to add firewall rules (`createFirewallRule`), open port forwards, configure load balancing, etc. **No new API calls are needed for these actions** – instead, CloudStack’s internal logic will route these operations to the VNF provider implementation for that network. For example, when a user calls `createFirewallRule` on a network that is flagged as using a VNF, the CloudStack management server will not instruct a VR to add an iptables rule (as it normally would), but will instead invoke the VNF provider’s logic to translate this into a call to the VNF appliance’s API. Thus, the existing API surface remains unchanged, which is beneficial for backward compatibility and for users (or tools like Terraform) who continue using the same CloudStack API calls. The difference is entirely in the back-end execution.
- **Dictionary Override and Updates:** If a user or admin wants to override the dictionary at the time of network creation (perhaps to provide custom behavior or a tweak for that network’s device), they can supply the YAML in the `vnfdictionary` parameter. We should also allow updating the dictionary for a given network after creation (e.g., `updateNetworkDictionary`), in case the VNF’s API changes or a correction is needed. However, altering the dictionary of a running network must be done cautiously – the system should validate it and possibly reconcile the rules after updating to ensure consistency.
- **Status and Testing APIs:** We might introduce some admin APIs to query the status of the VNF communication channel. For instance, an API like `testVnfConnectivity` could trigger a ping or API-call test to verify that CloudStack can reach the VNF (useful for troubleshooting). Similarly, `listVnfDevices` might list all VNF instances and their associated networks and templates for admin monitoring (though this information can be derived from existing calls like listing VMs of type VNF and their network attachments).
- **Reconciliation API:** As part of the drift management, a callable API `reconcileNetwork` (or specifically `reconcileVnfNetwork`) could be provided. When invoked on a network ID, it runs the reconciliation process for that network (compare CloudStack rules vs device rules and attempt to fix/flag). This might be admin-only and could return a summary of actions taken or discrepancies found.
- **Security and Secrets:** We may add APIs to manage secrets if not already present. CloudStack might have a concept of a “secret store” for credentials, or we might reuse existing mechanisms (for example, using the global settings or template details which are stored encrypted). If not existing, introducing something like `registerSecret` and `deleteSecret` accessible to admins could be useful to store API keys, etc., that then can be referenced in dictionaries. However, since the 4.21 template details already include fields for USERNAME, PASSWORD, etc., we can leverage those (they are stored encrypted in the DB by CloudStack by default?29†L43-L50?).
In summary, the API layer changes focus on enabling **template dictionary management**, **network creation with a VNF**, and **operational commands routing to the VNF**. Wherever possible, we reuse the existing API calls for network services so that from the viewpoint of an API client or end-user, nothing drastically new needs to be learned. The new APIs mainly serve administrators setting up the system (uploading dictionaries, etc.) and maintaining it (e.g., reconcile or test calls).
Service Layer & Orchestration Logic
With the new APIs defined, the core of the implementation lies in CloudStack’s management server logic that orchestrates VNF-backed networks. This involves coordinating resource creation (VR and VNF VM), managing the network state, and hooking into CloudStack’s network service workflows to call out to the VNF. We describe the end-to-end flow and responsibilities here:
- **Network Creation Workflow:** When a request to create a VNF-backed network comes in, CloudStack will:
  1. Allocate the network in the database with the specified network offering (which indicates use of VNF provider for certain services). Mark this network as having an associated VNF (store the `vnftemplateid` and possibly an `vnfdictionary` override reference).
  2. **Deploy the Virtual Router** for this network, but in a limited capacity. The VR is created as usual (as a system VM in the appropriate zone, attached to the guest network, and if needed a public network), but its configuration differs: it will not take the gateway .1 IP or set up source NAT or firewall rules as it normally would. Instead, the VR is only set up to provide DHCP, DNS, and metadata. For example, on an isolated network with range 10.1.1.0/24, normally the VR is at 10.1.1.1 and is the gateway. In a VNF network, CloudStack could choose to **reserve** 10.1.1.1 for the VNF appliance and configure the VR on a different IP (or configure VR with no gateway IP at all, just as a DHCP relay). One approach is to have the VR act as a DHCP server that hands out leases with the gateway option set to the VNF’s IP. CloudStack’s orchestration needs to handle this: possibly deploying VR with a custom user data script or configuration to not assume .1 as itself. (This might require tweaks in the VR template/agent code to allow non-standard gateway setup.)
  3. **Deploy the VNF Appliance VM:** CloudStack will then create a new VM from the specified VNF template. This involves selecting a host, using the given service offering (for CPU/RAM), and attaching the required NICs. The NIC mapping must match the template’s VNF NIC definitions (e.g., NIC0 might be the management/public interface, NIC1 the guest-lan interface, etc.). CloudStack will attach NICs accordingly: for instance, NIC0 on the public network (if the VNF needs a public uplink), NIC1 on the guest network (to serve as gateway for that network). If the zone doesn’t use a separate public network (some CloudStack setups use the guest network VLAN for external traffic via VR NAT), then the VNF might only have one NIC on the guest and rely on the VR’s public NIC for egress. In that case, the VNF would route outbound traffic to the VR – however, since we want the VNF to truly be the gateway, the likely design is that the VNF gets a public NIC or public IP directly. For now, assume the VNF has one NIC on the guest (for internal traffic) and one on the public network (for internet access), similar to how VR has two NICs.
  4. **Configure IPs and Routing:** Once the VNF VM is up, CloudStack must ensure it has the correct IP addresses:
     - Assign the guest network gateway IP (e.g., 10.1.1.1) to the VNF’s guest NIC. This could be done by CloudStack via the hypervisor (injecting IP config) or via the VR’s DHCP if the VNF is set to DHCP on that NIC. However, giving .1 via DHCP to the VNF can conflict if VR is also trying to use .1. A cleaner way: CloudStack directly sets static IP .1 on the VNF’s guest NIC (some integration might be needed, possibly using the hypervisor API to set static IP or by using ConfigDrive/Cloud-Init if the VNF template supports it). Another approach is to have the VR initially come up on .1, then hand off .1 to VNF – but that seems complex. It’s more logical that CloudStack never assigns .1 to VR in a VNF network.
     - If the VNF has a public NIC, allocate a public IP from the zone’s pool and assign it to that NIC. This is analogous to how VR gets a public IP. CloudStack can either use DHCP on the public network (if CloudStack manages a DHCP for public IPs) or directly configure the NIC. CloudStack knows the public VLAN and IP; since the VNF template likely expects to configure its own interfaces, we might just allow it to DHCP. Alternatively, treat it like acquiring an IP for a VM and doing static NAT – but since this VNF *is* the router, it might not need NAT in the same way (it will do NAT internally).
     - The VR’s DHCP service should distribute the VNF’s guest IP as the default gateway to other VMs in that network. DNS can still be the VR’s IP (the VR can forward DNS queries), or the VNF might provide DNS if desired (though typically VR can do it).
  5. **Finalize Network Setup:** Mark the network as Implemented with the VNF. At this point, we have:
     - A running VR (with limited services, possibly no default route).
     - A running VNF VM acting as gateway (with appropriate IPs).
     - The network offering’s services divided between VR (DHCP/DNS) and VNF (firewall, etc.).
- **Handling CloudStack Network Operations:** Once the network is up, any action that affects network services must be directed to the correct provider:
  - **Firewall Rules:** When a user adds a firewall rule (ingress rule on a public IP, or egress rule on the network), CloudStack will identify that the network uses the VNF provider for firewall service. Instead of adding an iptables rule on the VR, the management server will call the VNF provider’s `applyFirewallRule` logic. This logic will take the CloudStack rule parameters (e.g., allow TCP port 22 from 1.2.3.0/24 to VM IP 10.1.1.10) and map them to the dictionary. For example, the dictionary might specify that a firewall rule creation requires an API call to the VNF’s REST endpoint `/api/firewall/rules` with a JSON payload. The service layer will populate that payload: fill in the source CIDR, destination port, protocol, action (“allow” or “deny”) as required by the vendor format. It will then invoke the broker to send this request to the VNF. We’ll cover the broker interaction in the next section, but assume it returns a success or failure and possibly an ID for the new rule. On success, CloudStack can store the vendor’s rule identifier (if provided) in the CloudStack database, linking it to the CloudStack rule (perhaps in the `firewall_rules` table’s details). Storing the external ID is important for deletion: when the user later deletes the rule, we know what identifier to call on the VNF device.
  - **NAT (Port Forwarding / Static NAT):** Similar to firewall rules, port forwarding rules added via CloudStack will be translated to appropriate calls. For example, if a user sets up port forwarding of public IP X port 80 to internal VM 10.1.1.50 port 80, the VNF likely needs a NAT rule and possibly a security policy. The dictionary could define a `NAT.create` that takes the public IP (which CloudStack knows it allocated to the VNF or VR) and the internal IP/port, and calls the device’s API to configure the NAT. The management server collects the needed info (public IP, internal IP, ports) and triggers the call.
  - **VPN or Load Balancing:** If the VNF supports VPN or LB and the network offering expects those, CloudStack would similarly route those configurations. For instance, enabling a remote access VPN on a VNF-backed network would instruct the VNF to enable its VPN service (dictionary might have `VPN.create` mapping).
  - **Guest Network Operations:** Some operations like acquiring a new guest IP, or configuring static routes, might involve both VR and VNF. For example, if a user adds a static route or a policy-based route, CloudStack might need to apply it on the VNF (since it’s the router). At present, CloudStack doesn’t expose such features widely, but future enhancements (like service chaining) could involve instructing VNF to route to another VNF, etc.
  
  It’s important that the CloudStack management server keeps track of which services the VNF is handling versus the VR. The **service-provider model** in CloudStack helps here: when the network was created with a certain offering, internally CloudStack knows e.g. “Firewall -> VNFProvider, DHCP -> VirtualRouter, DNS -> VirtualRouter, LB -> VNFProvider (or none if not supported),” etc. The existing orchestration code (in classes like `NetworkManagerImpl` and various `Element` implementations) will be extended so that when an operation occurs, it delegates to the correct element. We will implement a new set of element classes, perhaps `VnfFirewallElement`, `VnfPortForwardingElement`, etc., or a unified `VnfNetworkElement` that handles multiple services. These will implement the same interfaces as the VR element does (like `applyFWRules`, `applyPortForwardingRules`, etc.), but will use the dictionary to perform actions.
- **Interpretation of Responses:** After CloudStack sends a command to the VNF (through the VR broker), it needs to handle the response:
  - On success, as mentioned, it may retrieve vendor-specific identifiers or status info. For example, a create rule call might return an object or message. The dictionary’s **response mapping** will guide how to parse it. CloudStack might simply look for an “OK” or a numeric ID. If an ID is returned, store it in the CloudStack DB (e.g., as `externalId` in a generic way).
  - On failure (say the VNF’s API returns an error code or message), CloudStack should translate that into a CloudStack exception or error message. We can map common errors to user-friendly messages if possible. The operation should be marked as failed (and perhaps retried or rolled back). For instance, if adding a firewall rule failed, CloudStack might not create the rule record at all, or create it but mark it in an error state that the UI can show.
  - Edge cases: If the VNF is unreachable or times out, CloudStack should treat it similarly to a network element failure – perhaps throw a `ResourceUnavailableException` for that network. The user could then see a warning that their network’s device is not responding. CloudStack might automatically schedule retries or require admin intervention (like ensuring the VNF VM is up).
- **Lifecycle Management:** Apart from rule handling, CloudStack should manage the lifecycle of the VNF appliance similarly to VR:
  - **Start/Stop/Restart:** If the user stops the VNF VM (or if CloudStack stops it during maintenance), we should consider what happens. Possibly, stopping the VNF means the network loses its gateway temporarily (and likely connectivity for guest VMs). CloudStack may want to prevent users from accidentally stopping a VNF that is acting as a network’s backbone. We might treat the VNF VM like a system VM in that sense (at least optionally pin it always on). Alternatively, allow stop/start but clearly document that it affects network connectivity.
  - **Reboot or Recreate:** If the VNF VM is destroyed (intentionally or by accident), CloudStack should be able to recreate it (similar to how it can recreate a lost VR). The network would still exist, so perhaps a new API or admin action “reset VNF appliance” would deploy a fresh one. Then CloudStack would have to push all the stored rules to the new device (which is where the reconciliation process or simply reapplying rules comes in handy). Designing for this ensures resilience.
  - **Upgrade:** If the user updates the VNF template (say a new version of the vendor software), one could deploy a new appliance and replace the old one. This process might be manual at first (destroy old, create new, reapply rules). In future, we could automate a rolling upgrade.
Throughout the service layer, a key principle is **maintaining CloudStack’s state as the source of truth**. The dictionary and broker calls are means to enforce CloudStack’s desired state on the VNF. CloudStack’s database will store what firewall rules should exist, what NAT mappings, etc., and we assume the VNF should mirror that. If at any point they diverge, CloudStack will rely on the reconciliation logic (discussed later) to detect and correct it. This approach ensures that even if a VNF has additional capabilities, CloudStack only cares about what it has been asked to manage; anything outside of that either isn’t touched or is removed if it conflicts.
By carefully orchestrating network creation and operation handling, the service layer ensures an **end-to-end delivery** of network functionality: from API call by user -> CloudStack translates via dictionary -> VR broker -> VNF applies it -> response back -> CloudStack updates state. Each component has a focused responsibility in this chain, which aids in clarity and debuggability.
VNF Broker Integration via Virtual Router
In this design, the Virtual Router (VR) system VM serves as a **communication broker** between the CloudStack management server and the VNF appliance. Leveraging the VR has multiple advantages: it is already deployed on the same host or network as the guest network, it’s under CloudStack’s control, and it can be updated with new software if needed. Here we describe how the broker function will work and be implemented on the VR:
- **Communication Channel Establishment:** We will establish a secure, authenticated channel between the CloudStack management server and the VR dedicated to VNF communications. In practice, CloudStack management already communicates with VRs via the agent management channel (the CloudStack agent runs inside VR for config). However, to keep this decoupled and secure, we introduce a separate channel using **HTTPS with mutual TLS**. The VR will run a lightweight web service (for example, an embedded Jetty server or even a simple Python Flask app) listening on a specific port (say 8443) on the VR’s guest network interface (or possibly on localhost and port-forward it via the agent). The CloudStack management server will act as a client to this service, opening an TLS connection where both sides present certificates. This ensures that even though the VR is just another VM, only the CloudStack management (with the right client cert) can talk to it. The use of mTLS guarantees identity: the management server trusts it’s talking to an authentic CloudStack VR, and the VR knows it’s receiving requests from the legitimate management server.
- **Broker API on VR:** The VR’s broker service will expose a simple **API endpoint** to receive VNF operation requests. For example, a POST request to `https://<vr-ip>:8443/vnfproxy` might be used. The payload of this request would contain all information needed to contact the VNF device and perform an action. This could be structured as JSON like:
  ```json
  {
    "target": "192.168.1.1",
    "protocol": "HTTPS",
    "method": "POST",
    "uri": "/api/firewall/rules",
    "headers": { "X-API-Key": "abcd1234" },
    "body": "{ "src": "0.0.0.0/0", "dstPort": 22, ... }"
  }
  ```
  This is an example for a REST call; for an SSH/CLI-based command it might include something like `"protocol": "SSH", "command": "set firewall rule ..."`. The management server, after rendering the dictionary, sends a fully-formed request description to the VR. **The VR does not need to understand the semantics** of the command; it just needs to execute it exactly as instructed.
- **Executing Requests on VR:** Upon receiving such a request, the VR’s broker service will:
  1. Authenticate the request (verify the mTLS client cert is valid and perhaps check an internal token as described below).
  2. Validate that the target IP is allowed (the VR should have a rule that it only connects to the known VNF IP for this network – more on that in the security section).
  3. Dispatch the request: For HTTP/HTTPS, the VR can use a tool like `curl` or a Python `requests` library to contact `https://192.168.1.1/api/firewall/rules` with the given method, headers, and body. For SSH, the VR might use `ssh` command or paramiko library to connect to the VNF’s SSH server and run the given command(s). The credentials or keys needed for SSH must be available to the VR (CloudStack can copy an SSH key to the VR or use password authentication if absolutely needed). These credentials wouldn’t be in the JSON payload in plaintext; rather, the VR might be pre-configured with the key (for example, CloudStack could inject the public key of the VR into the VNF VM on deploy, so that VR can SSH without password).
  4. Collect the response: If it’s an HTTP call, capture the HTTP status code and response body. If it’s an SSH command, capture the stdout/stderr and exit code.
  5. Return a response to CloudStack: The VR service will format a response (JSON) back to the management server, for example:
     ```json
     {
       "status": 200,
       "body": "{ "rule": { "id": 123, ... } }"
     }
     ```
     or an error if it couldn’t connect or the device returned failure.
  Alternatively, instead of an HTTP service on VR, we could implement this via the CloudStack agent messaging system: i.e., define a new `VnfBrokerRequestCommand` in CloudStack’s agent framework. When CloudStack wants to send something, it issues this command to the VR (over the existing agent link). We then modify the VR’s agent code to handle this command by performing the request and then replying with the results. This approach reuses the current communication mechanism (which is already secured by the agent’s SSL and authentication). The downside is having to extend the agent code and possible size limitations for payload. Still, it might be simpler given CloudStack’s architecture. The concept remains the same: VR is executing requests on behalf of management.
- **Future External Broker:** The broker concept is abstracted such that tomorrow, if an operator wanted to use an external controller instead of the VR, the management server could be configured to send these requests to a different endpoint. In that case, the VR might not even be needed for that network (the network offering could omit VR entirely if the external controller handles DHCP, etc., or we could still use VR for DHCP but not for control traffic). The CloudStack management’s **VnfBrokerClient** can have logic like: if `brokerType = VR`, send to VR; if `brokerType = external`, send to some external URL (with appropriate auth). For now, CloudStack 4.21.7 will implement only the VR broker, but building this abstraction in the code will ensure **extensibility**?26†L237-L242?.
- **Broker Security and Multi-Tenancy:** Each VR is associated with a particular network (or VPC). It should only broker requests for its own network’s VNF. We will enforce this by design:
  - The management server will know the VR ID or IP for the network and only send requests for that network’s VNF to that VR.
  - The VR service, upon receiving a request, could be configured with a preset allowed VNF IP (passed from CloudStack when the VNF was deployed or via the JWT token as described later). If a request tries to target an IP that is not that, it will reject it.
  - The VR does not store any long-term state about rules or dictionaries; it’s a stateless proxy. This is good for scalability – you could restart a VR or re-deploy it (in case of a failure) and the management server can continue sending requests. VR only needs the current credentials or keys to reach the VNF, which can be re-injected as needed.
- **VR Implementation Considerations:** We will update the VR **system VM image** to include any necessary tools:
  - Ensure `curl` is available (likely already is) for REST calls. Possibly add `jq` if we wanted to do JSON parsing on VR (though better to parse on the management side).
  - Include an SSH client and the ability to hold SSH keys. CloudStack might mount an ISO or use cloud-init to deliver an SSH private key for the VR to use when connecting to VNFs (or we generate a key pair for the VR at boot and share the public part with the VNF VM’s authorized keys).
  - If using a web service on VR: include a simple web server or extend the Python-based `CsAddress` agent that VR uses. We might incorporate a small Flask app in the VR that runs under `apache` or standalone. The overhead is minimal because this is only for control traffic, not high-frequency.
  - Because VR is a shared resource in the sense of code (all networks use the same template), we must ensure the broker service is robust and cannot be misused. The next section on security details mTLS, JWT, and firewall rules to lock this down.
Overall, the VR broker acts as a **trusted delegate** of the CloudStack management: it receives a fully-formed instruction and carries it out, similar to how the VR normally applies DHCP or firewall rules given by management. The difference is now VR might call out to another system (the VNF) to do so. By keeping the VR’s role generic (just pass through requests, enforce basic security), we avoid putting any vendor-specific logic on the VR – all that logic lives in the dictionary and management server. This aligns with keeping CloudStack’s core and system VMs vendor-neutral and maintainable.
Secure Communication and Enforcement
Security is paramount in this design because we are effectively opening a path for CloudStack to send arbitrary commands to a VM (the VNF) via another VM (the VR). Without proper safeguards, this could be abused. Thus, we implement multiple layers of security: mutual TLS for transport security, JWT-based authorization for fine-grained control, and strict network-level filtering on the VR. Additionally, we handle sensitive data (credentials) with care. Here are the details:
- **Mutual TLS (mTLS) between Management and VR:** Every communication to the VR’s broker endpoint will use HTTPS with client authentication. In practice, when a VR is created, CloudStack can generate a unique SSL certificate for that VR (or use a common certificate but unique keys). We might establish an internal Certificate Authority for CloudStack system VMs. During VR boot or configuration, CloudStack injects the server certificate (and private key) into the VR and also keeps a copy of the CA to trust. The management server has a client certificate (maybe one per management server or one per network) that the VR is configured to trust. This way, when management connects to `https://VR-ip:8443/vnfproxy`, the VR will verify the client’s cert is signed by our CA and corresponds to an authorized management entity, and the management verifies the VR’s cert as well. mTLS provides confidentiality (encryption of commands/results in transit) and mutual authentication. Even if an attacker were in the same guest network, they couldn’t eavesdrop or impersonate either side.
- **JWT Authorization for Requests:** In addition to the transport-level security, we embed a JSON Web Token in each request header (for example, in an `Authorization: Bearer <token>` header). This JWT is generated by the CloudStack management server just-in-time for the request. It is signed using CloudStack’s private key (or a symmetric key known to the VR if we prefer HMAC). The token will carry specific claims, for example:
  - Network or VNF identifier (to bind the token to a particular context).
  - Allowed target IP/hostname (the VNF’s management address).
  - Maybe the allowed operation type or a nonce.
  - An expiration time (very short-lived, e.g., valid for only a few seconds).
  
  When the VR receives a request, it will decode and verify this JWT (it will have the public key or shared key to do so). If the token is invalid or expired, the VR rejects the request. If valid, the VR then compares the token’s allowed parameters to the request it received. For instance, if the JWT says it’s for network X and target IP 192.168.1.1, but the request payload is trying to hit 10.5.5.5, the VR will refuse. This mechanism ensures that even if someone somehow could call the VR’s API (with a valid cert), they’d also need to produce a valid JWT for each action. CloudStack management will generate tokens such that one token = one operation, one target. This way, *each* command is self-contained and authorized. The VR doesn’t need to maintain session state; it just checks the token each time.
- **Per-Network Egress Allowlist on VR:** On the network level, we will configure the VR’s own firewall (iptables) to permit outbound traffic only to specific addresses:
  - The primary allowed address is the VNF’s management IP (or IPs) for that network. When the VNF VM is deployed, CloudStack knows its IP (say 192.168.1.1 on guest network). The VR can set iptables rules that allow connections from itself to 192.168.1.1 (on the needed ports, e.g., 443 for HTTPS, or 22 for SSH) and block all other outbound connection attempts. This prevents the VR from being used as a springboard to call arbitrary hosts.
  - In some cases, a VNF’s management interface might not be in the guest network but reachable via another path (less likely). Or perhaps the dictionary calls for contacting an external cloud service (for example, some VNFs might require reaching a vendor licensing server to apply a license). If so, we can extend the allowlist for those specific endpoints as needed, but these should be explicitly configured by an admin so they’re under control.
  - By default, after network setup, CloudStack will program the VR’s egress rules to only talk to the VNF VM. The JWT provides additional protection by specifying the same IP; the iptables provides a hard fail-safe at the OS level.
  - Additionally, the VR can restrict *incoming* connections to its broker port to only the management server’s IP (if reachable) or generally to require mTLS (which it already does). Usually, the management server is on a different network (management network) not directly reachable to VR’s guest NIC, so actually the connection might be going via the host or port forwarding. If the VR initiates connection (in case we used agent commands), that’s internal, but for an HTTP listener approach, we may end up using the agent as a proxy or have to expose the port on VR’s interface. We will ensure it’s not open to the world by security groups or other means, depending on zone configuration.
- **Secret and Credential Security:** As noted earlier, any secrets (API keys, passwords) are not stored in the dictionary itself. Instead, they reside in CloudStack’s secure storage:
  - CloudStack’s database already encrypts sensitive values like user credentials and VPN secrets?29†L43-L50?. We will make sure the VNF template details that hold `USERNAME`, `PASSWORD`, `SSH_PASSWORD`, `API_KEY`, etc., are marked as sensitive so that they get encrypted. (If CloudStack’s VNF implementation already did this for those fields, it likely does since they are akin to passwords.)
  - When CloudStack needs to use a secret, it will retrieve it from the DB (decrypt in memory) and insert into the request. For example, if the dictionary says a header “X-Token: ${tokenRef:MYTOKEN}”, CloudStack will fetch the value of secret “MYTOKEN” (which might correspond to something stored when the template was configured, such as an API token string), then populate that header.
  - The VR, when proxying, will see the final header value or credential. We trust the VR since it’s under our control and isolated. However, the VR should not log these values. We might need to scrub or avoid verbose logging on VR for the proxy actions, especially if something fails – logs should not dump entire payloads with secrets.
  - The communication between management and VR is encrypted (mTLS) and between VR and VNF can also be encrypted (if using HTTPS/SSH). So the secrets are protected in transit.
  - We also avoid storing secrets on the VR disk. If the VR needs an SSH key to connect to the VNF, that key is placed in memory or a protected file and can be unique per network to limit exposure. CloudStack could generate a fresh key pair for each VNF network’s communications, so even if one is compromised, it doesn’t affect others.
- **Verification and Auditing:** Every action that CloudStack triggers on a VNF can be logged (at least at a high level) for auditing. For instance, CloudStack can log an event “Applied firewall rule ID 123 on VNF (template X) in network Y”. We may not log the full request body (to avoid clutter and secret leakage), but an identifier and result code can be recorded. The VR can also keep minimal logs like “Received request to connect to VNF IP X – success/fail”. In case of issues, an admin can enable debug which might log more details temporarily.
By combining **mTLS** (secure transport) with **JWT authorization** (per-request validation) and **strict firewall rules**, we create a multi-layer defense where:
1. Only CloudStack management can talk to VR’s broker.
2. Only requests containing a valid signed token for a specific VNF will be honored.
3. Even if malicious data got through, the VR cannot reach unintended systems.
4. Secrets are tightly handled and never exposed in plaintext unnecessarily.
This security model ensures that the new feature does not introduce undue risk to the cloud environment. Even tenants cannot subvert it to, say, configure someone else’s VNF or access things they shouldn’t. All actions remain under the control of CloudStack’s authenticated API and admin governance.
VNF Dictionary Format and Processing
At the heart of the VNF framework is the **YAML dictionary** that defines how CloudStack should translate generic network operations into vendor-specific API or CLI calls. This dictionary is essentially a form of driver definition in data form. It allows CloudStack to support many different VNF vendors without hardcoding their logic – by simply loading a new dictionary, CloudStack’s behavior can adapt to a new device’s API. It’s crucial to get the structure of this dictionary right. Below we describe the expected format and how CloudStack will use it:
**Top-Level Structure:** The dictionary is a YAML (human-readable and easily editable) document. Major sections include:
- **`version`:** (Optional but recommended) indicates the version of the dictionary schema or format being used (e.g., `1.0`). This helps with future schema evolution and validation.
- **`vendor` / `product` / `version`:** (Optional) meta-information about the VNF vendor or appliance this dictionary is intended for. For example, `vendor: PaloAlto`, `product: PAN-VM`, `firmwareVersion: 10.2`. CloudStack might not use this internally except for logging or UI display, but it’s useful for administrators to tag what this dictionary is for.
- **`access`:** This section details *how to connect* to the VNF’s management interface. It typically includes:
  - `protocol`: Could be `ssh`, `telnet` (unlikely), `http`, `https`. This tells us which connector to use.
  - `port`: The port to connect to (e.g., 22 for SSH, 443 for HTTPS).
  - `path` or `basePath`: If the VNF’s API resides under a certain URL path (for REST APIs). For example, some appliances might require all requests to prefix `/api/v1`.
  - `credentials` or auth method: This can be further broken down:
    - If `protocol` is SSH, we might need `username` and either `password` or an `sshKey` reference.
    - If `protocol` is HTTPS and the API uses basic auth, we need `username` and `password` (likely references to secrets).
    - If token-based, perhaps an `authType: token` and a `tokenHeader` (e.g., `X-API-Key`) and a reference to the token value.
    - Or if no auth is required (maybe the device allows API calls from the VR’s IP without auth), it could be `authType: none`.
  - We may incorporate these as standard detail names, or allow arbitrary fields under access for flexibility. For consistency, we could define names like those already present in CloudStack’s VNF template details (e.g., `SSH_USER`, `SSH_PASSWORD`, `API_KEY`, etc.) and have the dictionary reference them. For example:
    ```yaml
    access:
      protocol: https
      port: 443
      authType: token
      tokenHeader: X-API-Token
      tokenRef: API_KEY   # references the template detail or secret named API_KEY
    ```
    Or:
    ```yaml
    access:
      protocol: ssh
      port: 22
      usernameRef: SSH_USER
      passwordRef: SSH_PASSWORD
    ```
    The dictionary can thus refer to keys that CloudStack will look up in the VNF template’s details or secret store. This separation ensures the actual sensitive values aren’t in the dictionary file.
- **`services`:** This section is a mapping of network service names to their operations. Each **service** corresponds to a category of functionality that CloudStack might manage. Common services we expect:
  - `Firewall`
  - `NAT` (which might include both port forwarding and outbound SNAT rules)
  - `LoadBalancer`
  - `VPN`
  - `Routing` or `StaticRoutes` (less likely initially)
  - Possibly `DHCP` or others if the VNF also takes over DHCP (but in our current design, VR handles DHCP, so we probably don’t need that in dictionary).
  
  For each service, there will be sub-entries for operations. Typically:
  - `create` – how to add a new rule/entry for that service.
  - `delete` – how to remove an entry.
  - `list` – how to retrieve the current entries (used in reconciliation).
  - We might also have `update` if modifying rules is supported (CloudStack usually does remove+add for rules rather than in-place update, but VPN user lists or LB pool members could be updates).
  - Possibly `enable/disable` for services like turning on VPN or turning off something globally on the device (e.g., enabling the firewall feature if needed).
  Each operation entry contains the details of the API/command to call:
  - **For REST/HTTP APIs:**
    - `method`: HTTP method (GET, POST, PUT, DELETE, etc.).
    - `endpoint`: The URI path to call. This could include placeholders for identifiers. For example, in a Firewall `delete` operation, `endpoint: /firewall/rules/${externalId}` means CloudStack will replace `${externalId}` with the actual ID of the rule on the device (which it stored from the create step).
    - `body`: For methods like POST/PUT, the request body. This can be a string block or a YAML object that CloudStack will translate to JSON or XML. Within the body, placeholders will represent variables. For instance, `"src": "${sourceCidr}"` will be replaced with the actual CIDR string from the CloudStack rule. We might allow simple expressions or formatting, but initially placeholders mapping one-to-one to CloudStack context fields suffice.
    - `headers`: If any specific HTTP headers need to be sent for this operation (some APIs might require setting content-type or custom headers beyond the auth token).
    - We can also allow an `urlParams` or query string if needed.
    - Optionally, an `successCode` field to denote what HTTP status is considered a success (e.g., 200 or 201, etc., if not obvious).
  - **For SSH/CLI commands:**
    - Instead of method/endpoint, we might use a `command` field (or a list of commands) to run on the CLI.
    - Example:
      ```yaml
      services:
        Firewall:
          create:
            command: |
              configure firewall add rule name ${ruleName} src ${sourceCidr} dst ${destIp} port ${destPort} accept
            prompt: "#"
            successPattern: "Rule added successfully"
      ```
      This hypothetical snippet means: open an SSH session, maybe enter configuration mode, execute the given command. The `prompt` might define what prompt to expect (for sending commands in sequence), and `successPattern` could be a regex or string to look for in output indicating success. CLI integration is trickier due to interactive nature, but many network OS have non-interactive modes or single-shot commands that apply immediately.
    - If a device has a CLI configuration mode, we might need multiple commands (enter config mode, do command, commit). The dictionary format could allow a list under `command` to send multiple lines, or a single string with `
`.
    - The `responseMapping` for CLI could include a way to parse output or just a success/failure indicator.
  - **Response Mapping:** Each operation can specify how to handle the response from the VNF:
    - `responseMapping` can be a structure. For REST, likely:
      - `successCode`: (as mentioned) the HTTP code that means success (if different from 200).
      - `idPath`: a JSONPath or pointer to where the new object’s ID is in the response. For example, after creating a rule, the device might return `{"rule":{"id":42,"status":"OK"}}`; an `idPath: $.rule.id` tells CloudStack to extract `42` as the externalId.
      - Or for a list operation: `listPath` indicating where the array of entries is in the response, and then some mapping of fields. Perhaps:
        ```yaml
        list:
          method: GET
          endpoint: /firewall/rules
          responseMapping:
            listPath: $.rules
            item:
              idPath: $.id
              srcPath: $.src
              dstPath: $.dst
              portPath: $.port
        ```
        CloudStack would GET `/firewall/rules`, get back JSON with "rules": [...], then for each item in that list use the sub-paths to construct a comparable record of what the device has. This is useful for reconciliation. We may not need to map every field, just enough to identify the rule (id and maybe the main properties).
      - If an API returns a different structure or needs parsing, we may need flexibility (maybe allow JMESPath or JSONPath expressions).
    - For CLI/SSH, `responseMapping` might include:
      - `successPattern` or exit code expectations.
      - Possibly capture groups from output if needed (though ideally we keep CLI devices simpler: e.g., no need for an external ID because we can often identify rules by parameters or have the device list them).
    - If the dictionary doesn’t provide an explicit mapping, CloudStack will use default assumptions (e.g., HTTP 200 is success, no ID to store unless specified).
- **Example Dictionary Outline:** To illustrate, here’s a pseudo YAML snippet combining many of the above ideas for a fictional device:
  ```yaml
  version: 1.0
  vendor: ExampleCorp
  product: SuperFirewall
  access:
    protocol: https
    port: 8443
    authType: basic
    usernameRef: WEB_USER
    passwordRef: WEB_PASSWORD
    basePath: /api/v1
  services:
    Firewall:
      create:
        method: POST
        endpoint: /firewall/rules
        body: |
          {
            "name": "${ruleId}", 
            "srcIp": "${sourceCidr}", 
            "destIp": "ANY", 
            "port": ${publicPort}, 
            "protocol": "${protocol}", 
            "action": "allow"
          }
        responseMapping:
          successCode: 201
          idPath: $.ruleId
      delete:
        method: DELETE
        endpoint: /firewall/rules/${externalId}
        successCode: 204
      list:
        method: GET
        endpoint: /firewall/rules
        responseMapping:
          listPath: $.rules
          item:
            idPath: $.id
            srcPath: $.srcIp
            portPath: $.port
            protoPath: $.protocol
    NAT:
      create:
        method: POST
        endpoint: /nat/portforwards
        body: |
          {
            "publicIp": "${sourceIp}", 
            "publicPort": ${publicPort}, 
            "privateIp": "${destIp}", 
            "privatePort": ${destPort}, 
            "protocol": "${protocol}"
          }
        responseMapping:
          idPath: $.natId
      delete:
        method: DELETE
        endpoint: /nat/portforwards/${externalId}
        successCode: 204
      list:
        method: GET
        endpoint: /nat/portforwards
        responseMapping:
          listPath: $.rules
          item:
            idPath: $.natId
            publicIpPath: $.publicIp
            publicPortPath: $.publicPort
            privateIpPath: $.privateIp
            privatePortPath: $.privatePort
  ```
  This example shows how firewall and NAT rules might be defined. Placeholders like `${publicPort}`, `${protocol}` correspond to CloudStack rule attributes (publicPort could be the port on the public IP for port forwarding, etc.). We also have an `${ruleId}` – CloudStack could generate a unique name or use its internal rule ID as a reference if the device needs a name. `${externalId}` is used in delete, which CloudStack will have filled in from when the rule was created. 
- **Dictionary Validation:** CloudStack will need to validate dictionaries when they are uploaded:
  - Check that required sections exist (`services` must be present, etc.).
  - Check that for each service, if `create` is defined, a corresponding `delete` is likely defined (otherwise we can’t remove things).
  - Verify no obviously wrong placeholders (like a placeholder with no matching data in CloudStack context).
  - We might ship a JSON Schema for the dictionary format and run it against the YAML. This schema can evolve with `version`. If `version: 1.0`, we know what keys are expected.
  - If the dictionary is not valid, the API should reject it with an error, so the user can fix the YAML.
- **Versioning and Backward Compatibility:** By including a `version` field, we allow the format to evolve. For instance, version 2.0 might introduce a new field or deprecate something. CloudStack’s parser can check `version` and apply the right interpretation. We can maintain backward compatibility by continuing to support 1.0 dictionaries even if 2.0 offers new features. This means not making breaking changes without good reason. Instead, add new fields in a way that if they’re missing, we have a default behavior.
  - For instance, if we later add an `update` operation support, a 1.0 dictionary won’t have it – CloudStack should handle that gracefully (maybe by doing delete+create internally if update is invoked, or by refusing update with a clear message).
  - We might eventually formalize the dictionary schema (could even publish a JSON Schema or in docs so that vendors can write their own dictionaries easily).
  - It’s also wise to include a CloudStack version check. Perhaps store a minimum CloudStack version that the dictionary is intended for, especially if later we add things that older CloudStack code wouldn’t understand. But since we embed version, that might suffice.
- **Merging/Overriding:** If a network override dictionary is provided, CloudStack could either treat it as a full independent dictionary or as a patch on the template’s dictionary. A simple approach is to treat it as full (the user should include everything). But we could allow an override YAML that just has, say, one service’s modifications. To implement merging, we’d load the template’s dict and then overlay the override (e.g., deep merge). This is a nice-to-have; initially, requiring a complete dictionary override might be acceptable to simplify.
- **Internal Representation:** Once loaded, the YAML dictionary will be turned into an in-memory representation (objects or maps). The VNF provider logic will use this to find what to do for each action. For example, when `applyFirewallRule` is called, CloudStack code will do:
  1. Look up `dict.services.Firewall.create`.
  2. Take the CloudStack `FirewallRule` object and plug its fields into the template (this might use a simple templating engine or manual string replace for placeholders).
  3. Gather any additional needed data (like if we need the VM’s IP or network ID, CloudStack has that context).
  4. Construct a `VnfRequest` data structure that encapsulates all the info (method, url, body, etc.).
  5. Pass that to the broker client to execute.
  6. On return, check `dict.services.Firewall.create.responseMapping` to interpret the result, store external ID if present, etc.
- **Support for Multiple Formats:** While YAML is the format of the dictionary itself, the vendor’s API might use JSON or XML. The dictionary above assumed JSON in payload and response. If a device uses XML (some old appliances might), we could either allow the body to be an XML string and the responseMapping to use XPath. That complicates the parser a bit but is doable. Alternatively, require that any XML-speaking device be driven via CLI or some other interface instead, to limit complexity. However, for completeness, we can include, say, `responseMapping.xmlPath` or something if needed. The architecture diagram indicated support for JSON, XML, and plaintext handlers, so we will keep the code flexible to handle those.
In essence, the dictionary provides a **blueprint** for CloudStack-to-VNF integration. By following the user-specified structure with `access`, `services`, and per-service operations like `create/delete/list`?26†L179-L183?, CloudStack can systematically convert high-level operations into device-specific actions. This design means adding support for a new vendor is as simple as writing a new dictionary (assuming the vendor’s capabilities align with CloudStack features). It decouples the vendor logic from code – making maintenance easier and allowing quick updates (e.g., if a vendor changes their API, an updated YAML can be applied without a full CloudStack upgrade). It’s critical that we thoroughly document this schema for users and also handle errors gracefully when a dictionary is incomplete or incorrect, to aid troubleshooting.
Secrets Management and Injection
The VNF framework will often need to handle credentials – API keys, usernames, passwords – that allow CloudStack to authenticate to the VNF appliance. Instead of hardcoding these in the integration or (worse) including them in plain text in the YAML dictionary, we utilize CloudStack’s secure secrets storage mechanisms. Here’s how we manage secrets:
- **Using CloudStack’s Existing Secret Store:** CloudStack encrypts sensitive information such as passwords and secret keys in its database?29†L43-L50?. The VNF feature will piggyback on this capability. In CloudStack 4.21, there may not be a user-facing “Vault” UI, but the underlying storage is there via the `configuration` table encryption and template details. We will use **VNF Template Details** to store credentials whenever possible:
  - For example, when registering or updating a VNF template, the user can set details like `SSH_USER`, `SSH_PASSWORD`, `WEB_USER`, `WEB_PASSWORD`, `API_KEY`, etc.?20†L181-L189?. These are likely stored encrypted (passwords definitely, API keys hopefully too if marked as such).
  - If CloudStack doesn’t automatically encrypt a new type of detail, we will ensure to mark any sensitive field appropriately in code (some CloudStack APIs treat fields named “password” specially).
  - Additionally, CloudStack might allow storing an SSH private key (for key-based auth) – possibly not currently, but we could treat an `SSH_KEY` field as a secret and store it (perhaps base64 encoded and encrypted in DB).
- **Associating Secrets with Networks:** In most cases, the credentials to talk to a VNF are the same for all instances of a given template (e.g., default username/password for the appliance, or an API key that’s unique per instance but often you set it after deploy). If there is something unique per network (like maybe each VNF VM might generate a one-time password on first boot), CloudStack would need a way to capture that. A potential solution: when deploying the VNF appliance, if it has user data or some known default, the admin could update the template’s detail to the actual creds. Alternatively, CloudStack could support *per-network* secret overrides. This might be overkill initially. We assume template-level secrets suffice (the operator can bake in default creds or use ones that they set for all instances of that template). If needed, an admin can log into the VNF and change the creds, then update CloudStack’s records for that network’s appliance.
- **Injection at Render Time:** When CloudStack is about to send a request to the VNF (via VR), it will insert the needed credentials:
  - For an HTTP API: If the dictionary’s `access` says `authType: basic` with `usernameRef` and `passwordRef`, CloudStack will retrieve those values (decrypt them in memory) and then generate an `Authorization: Basic ...` header. It will do the Base64 encoding of `username:password` itself – the dictionary could also specify if needed (but we know how Basic auth works, so we can handle that automatically when we see authType basic).
  - For token auth: If `tokenRef: API_KEY` is given, CloudStack finds the API_KEY in the template’s details (or secret store) and then sets the header as specified (e.g., `X-API-Token: <actual token>`).
  - For SSH: If using password-based, CloudStack fetches the password and the VR will use it to login (maybe by echoing it into ssh command or using an SSHpass utility, though that’s not ideal security-wise; better use key). If using key-based, CloudStack ensures the VR has the private key to use (this likely means at template registration time, an admin provides an SSH key pair for the appliance’s access – CloudStack could store the private key encrypted, and store the public key somewhere accessible. When the VNF VM is deployed, CloudStack could inject the public key into it via cloud-init or the hypervisor user data, so that VR can later authenticate).
  - Note: We must ensure that these secrets are not present in the YAML dictionary. The dictionary only has references like `API_KEY` or `SSH_PASSWORD`. Users uploading dictionaries should know to use those placeholders. If a user did put an actual password in the YAML, CloudStack can’t automatically know and would treat it as literal text to send (which is functional but insecure). Ideally, our UI/UX should discourage that by making it easy to use references. The template detail UI might allow uploading secrets and then the dictionary can reference them.
- **No Secret in Logs:** Anywhere in the path from management server to VNF, we should scrub secrets from logs:
  - Management server logs: When logging a request being sent to VR, do not print full headers/bodies that include credentials. We can log what operation and which device, but not the auth tokens.
  - VR logs: If using an HTTP proxy approach and VR logs requests, ensure it doesn’t log the Authorization header. If we control the code (like in a Python script), we simply avoid printing it.
  - If an error occurs (e.g., authentication failure), log just “authentication failed” rather than printing the credential.
- **UI and API for Secrets:** In the UI, when the user is editing VNF template details, fields like PASSWORD or API tokens should be masked. Likely Primate already does this for template “password” fields. If not, we add appropriate UI hints. We may also implement a dedicated secret storage UI in the future, but for now template details suffice.
- **Runtime Secret Usage Example:** Suppose the dictionary says to call a REST API that requires an API token in a header. The steps are:
  1. Admin has stored the API token in the template’s details under name “API_TOKEN” (and perhaps set `access_methods` to include “https” so CloudStack knows to open port 443, but anyway).
  2. User creates a network with this template. The token is inherited since the same template is used.
  3. When a firewall rule is added, CloudStack finds the dictionary entry requiring X-API-Token header with `tokenRef: API_TOKEN`. CloudStack looks up the value of API_TOKEN (probably in a `template_details` table linked to the template, or possibly network_details if override).
  4. CloudStack pulls it (decrypting using its DB secret key) and gets the plaintext token (say “abc123XYZ”).
  5. CloudStack forms the HTTP request with header `X-API-Token: abc123XYZ` and body etc., and sends to VR via TLS.
  6. VR proxies it to the VNF over HTTPS. The token is now in transit to the VNF (encrypted in TLS on that leg too).
  7. The VNF validates token, does the needful, returns response. The token value isn’t returned back (and if it were, we’d treat it as any other data).
  
  At no point is the token stored in any log or persistent file in plain text. If someone were to compromise the CloudStack DB, they’d still need the master key to decrypt it (which CloudStack has in memory). It’s a trade-off: CloudStack does need to handle the secret to do its job, but we contain it as much as possible.
- **Rotation and Expiry:** If a secret (like an API key) needs rotation, the admin can update the template detail with the new key. Perhaps also update any running networks that used an override. CloudStack could then use the new key for subsequent operations. If the old rules on the device were created with the old key, that typically wouldn’t matter because the rules remain configured. The key is only for API access, not for the data plane. So rotation is straightforward – just update and CloudStack will use the new one for the next API call. It might be wise to test connectivity after updating a credential (a “Test” button can be provided, as discussed in UI).
In summary, the approach is **never embed secrets in the dictionary**, store them encrypted in CloudStack’s DB (leveraging existing encryption features), and retrieve/inject at runtime. This ensures that if someone obtains the dictionary (which might be visible to many users or checked into source control for custom integrations), it doesn’t contain the keys to the kingdom. Only the CloudStack admin who set the credentials in the system knows them, and CloudStack handles them in memory thereafter. This design satisfies the requirement of keeping secrets out of YAML and provides a secure way to manage credentials.
Reconciliation and Drift Management
Complex systems can drift out of sync – especially when an external device (the VNF) is involved. A user might log into the VNF’s console and change a setting, or a CloudStack operation might fail halfway, or perhaps CloudStack was down and the device config changed. To ensure CloudStack’s view of network state remains accurate, we implement a **reconciliation (sync) process**. This will periodically compare CloudStack’s intended state (from its database) with the actual state on the VNF, and attempt to correct any discrepancies or at least flag them for administrators.
- **Need for Reconciliation:** CloudStack treats itself as the source of truth for network rules and config it manages. However, since the VNF could also be configured outside of CloudStack (intentionally or accidentally), it’s possible the device has rules that CloudStack didn’t create or is missing rules that CloudStack thinks should be there. Also, in the event of failures (like network hiccup during an API call), CloudStack might have marked a rule as added when it wasn’t, or vice versa. Reconciliation addresses these by verifying reality.
- **Scheduled Job:** We will introduce a background job (could be part of CloudStack’s existing periodic tasks) that runs reconciliation at a configurable interval (say every 15 or 30 minutes). This job will iterate over all networks that have VNF integration (or possibly all VNF appliances). We might scope it by network offering or a flag in the network object indicating a VNF-backed network. For each such network, it triggers a reconcile routine. We should make the interval configurable and possibly allow disabling automatic reconcile if needed (for performance considerations on very large deployments or very large rule sets – though the default should be enabled to maintain integrity).
- **On-Demand Reconcile:** In addition to the periodic task, an admin or even a user (with proper rights) should be able to trigger a reconcile on a specific network via API or UI (e.g., pressing a “Reconcile” button or calling `reconcileVnfNetwork` as mentioned before). This is useful after making manual changes on a VNF or after fixing a connectivity issue to get things back in sync immediately.
- **Reconciliation Process for a Network:**
  1. **List Device State:** For each type of service that CloudStack manages on the VNF, CloudStack will call the **`list` operation** defined in the dictionary. For example:
     - Call the firewall `list` API on the VNF (via VR) to get the list of all firewall rules currently on the device.
     - Call the NAT `list` API to get all port forwards or NAT entries.
     - Similarly for VPN or LB, if applicable.
     These calls use the same secure channel. The dictionary’s `responseMapping` tells CloudStack how to parse the output into a set of entries.
  2. **Fetch CloudStack State:** CloudStack then retrieves its own record of what should be on the device:
     - The list of firewall rules the user created for this network (that are in state “Active” or “Implemented” in CloudStack’s DB).
     - The list of NAT rules (port forwarding rules, static NATs, etc.) that are configured in CloudStack for this network.
     - And so on for other services (e.g., VPN users or LB rules if managed).
     CloudStack can query its database or use existing API logic to get these (since we are inside management, direct DB or service layer queries are fine).
  3. **Compare:** Now, for each service type, compare the two sets:
     - Find **extra entries on device** that CloudStack doesn’t have in its DB. These could be rules someone added via the VNF’s own UI/CLI. For instance, CloudStack knows of 3 firewall rules, but the device’s `list` returns 5 rules – two are unknown to CloudStack.
     - Find **missing entries on device** that CloudStack believes should be there. For example, CloudStack has a firewall rule ID 10 in its DB, but the device `list` doesn’t show a corresponding rule. This could mean the rule was never applied or was removed outside CloudStack.
     - In comparing, use identifying attributes. If the device returns an ID for each rule, CloudStack can use that to match (if it stored those IDs). If not, CloudStack might compare by rule properties (like same protocol/port/CIDR). Some differences might not be 1-to-1: e.g., a device might split one CloudStack rule into multiple internal entries (though usually we design so it’s 1-to-1).
  4. **Repair or Report:**
     - For **missing entries** (CloudStack wants it, device doesn’t have it): CloudStack should attempt to **reapply** them. It can call the `create` operation again for that rule. Perhaps mark that it’s a reconcile action (so that if it fails, we log appropriately). Ideally, this is done carefully to avoid duplicate entries; but if the device truly doesn’t have it, adding it is correct. If the device had partially applied it (maybe under a different ID), there could be a conflict – we rely on the device’s API to handle duplicate gracefully (it might return an error like “already exists” which we can treat as success or ignore).
     - For **extra entries** (device has something CloudStack doesn’t know): CloudStack has two choices. One approach is to simply report it (i.e., log and perhaps raise an event “Device has unknown firewall rules that CloudStack did not create”). Another approach is to remove them to enforce consistency. Removing blindly could be problematic if someone intentionally added a rule outside CloudStack – but ideally all changes should go through CloudStack. Since CloudStack is meant to be the orchestrator, it’s reasonable to remove extraneous rules to maintain a clean state, *especially* if they might interfere with intended policies. We might implement a conservative strategy at first: flag the discrepancy in the UI/events and optionally allow an admin to trigger removal. Perhaps an advanced configuration flag can allow automatic cleanup. For the initial implementation, it’s safer to **flag drift** rather than automatically delete unknown rules, unless they are clearly conflicting with a CloudStack rule.
     - CloudStack can also reconcile differences in content. For example, if a CloudStack firewall rule says allow 80/tcp from 1.2.3.0/24 but on the device it’s 0.0.0.0/0 (broader), that’s drift. Ideally, CloudStack would correct it by narrowing it (delete and re-add correctly). But that requires detecting such subtle differences which might be complex. Possibly the device’s `list` output might not give exactly the same structure (some might combine rules, etc.). We handle straightforward cases reliably, complex mismatches can just be flagged.
  5. **Record the Outcome:** After attempting fixes:
     - If everything matches or was fixed, we can log a success like “Network X reconciled: 2 missing rules applied, 0 discrepancies remaining.”
     - If some issues couldn’t be fixed (e.g., removal of unknown rule was not done automatically), log what remains out of sync.
     - CloudStack could raise an event or notification for admin. Perhaps the network gets a flag “Drift detected” which can be shown in UI until cleared.
     - If using UI, we might list unknown rules with an option “adopt” or “remove”. “Adopt” meaning the admin decides to import that rule into CloudStack’s state (though we have no API to directly create a rule with exact parameters without user input – could mimic by calling create rule with those params if allowed). But adoption is advanced, likely we skip that.
     
- **Resiliency:** The reconcile job should handle cases where the VNF is not reachable (maybe it’s down or network issues):
  - If we can’t contact the device at all, log an error “Unable to reconcile network X: VNF unreachable”. Perhaps set a flag so that if multiple cycles fail, raise an alert.
  - It should not hang indefinitely – use timeouts on the `list` calls.
  - If a device returns a huge list of entries (say 1000 rules), ensure we handle that (maybe our internal structures or loops should be optimized, but 1000 is not crazy).
  - Also, do one network at a time to not overload any particular VR or the management (some throttling if needed).
- **Consistency with CloudStack DB:** If CloudStack finds an extra device rule and we choose to do nothing automatically, we have the scenario that CloudStack doesn’t know about it. That’s generally okay – CloudStack doesn’t have an object for it, and unless it conflicts, CloudStack’s operations won’t consider it. But it might allow traffic that CloudStack thinks is blocked, which could be a security concern. That’s why probably removal is better. On the flip side, if CloudStack finds a missing rule and re-adds it, then CloudStack was already assuming it’s active, so bringing it back enforces expected security.
- **Other Drift Types:** Reconciliation can also cover:
  - The VNF’s interface IPs: e.g., ensure the VNF still has the right IP on the guest network (perhaps not needed unless someone manually changed it).
  - The device’s global status (if say someone disabled the firewall feature on the device, CloudStack wouldn’t directly know, but when adding rules it might notice).
  - If the VNF VM is powered off, reconciliation might note “device is down” rather than any rule drift.
  - If multiple VNF devices in chain (future), reconciliation might chain through them, but we’re not there yet.
- **UI Indication:** In the UI, on the network detail panel, we can show something like:
  - Last Reconcile: timestamp
  - Drift Status: OK / Drift Detected
  - If drift, perhaps a small link or popup to view details (like “2 unknown rules present on device”).
  This gives admins immediate insight if their CloudStack and VNF are not aligned.
- **Example Scenario:** Suppose an admin manually added a firewall rule on the VNF to allow some debugging port, outside of CloudStack’s knowledge.
  - Reconcile runs, gets device rules including that one, sees CloudStack has no matching rule.
  - CloudStack could log: “Detected rule on device not present in CloudStack: allowing port 9999 from 10.0.0.0/8. No action taken.” and mark drift.
  - If automatic removal were enabled, CloudStack would call delete for that rule (perhaps it needs to identify it by ID or by matching unique fields).
  - Next time reconcile runs, it should ideally see no drift.
  - Alternatively, if a CloudStack rule failed earlier and the device lacks it, e.g., CloudStack had a rule to allow 8080 but device doesn’t have it:
    - Reconcile calls create for 8080 rule again. If succeeds this time, great. If it fails again (maybe device says “duplicate” because maybe it was there but under different ID?), we may need to handle that. Possibly an API returning duplicate error could be treated as success if we can confirm the rule actually exists (maybe it exists with different ID that CloudStack didn't catch due to a missed response earlier).
    - This is a corner case – ideally if CloudStack missed storing the ID but the device did apply it, the list would show it (so CloudStack would think it's unknown since no DB entry mapping – but actually CloudStack does have a DB entry for rule, just no externalId). In that case, the device rule matches a CloudStack rule by properties, we could then just update CloudStack’s record with the found external ID rather than create new. That level of intelligent match might be complex, but it’s something to consider (matching by protocol/ports likely).
  
- **Impact on Device:** Frequent reconciliation means additional API calls to the device. This should be fine for most, but if a device is very constrained or has a lot of rules, listing them often might load it. We should allow tuning the frequency or disabling it if needed.
In conclusion, reconciliation is the safety net that ensures **configuration drift is minimized**. It gives CloudStack operators confidence that the VNF devices are doing exactly what CloudStack thinks they are. By automatically correcting divergences or flagging them, it reduces the risk of unseen misconfigurations. This feature effectively makes VNFs first-class citizens managed under CloudStack’s desired-state paradigm, rather than black boxes that could diverge over time.
UI and Primate Integration
Implementing the VNF framework isn't just about backend services; we must also extend CloudStack’s UI (Assuming the Primate UI is in use for 4.21+) to expose these features in a user-friendly way. The UI will cover two main areas: **Template management for VNFs (including dictionary upload)**, and **Network management for VNF-backed networks (including creation and monitoring)**. All changes should fit naturally into CloudStack’s existing interface paradigms for consistency.
VNF Template UI (Dictionary Management)
In the Templates section of the CloudStack UI, templates of type "VNF" will have additional options:
- **VNF Settings Tab:** As noted in CloudStack 4.19/4.21, when you view a template there is a "VNF settings" tab where you can configure NIC mappings and details?19†L146-L154??19†L175-L183?. We will extend this tab (or add a sub-tab) to manage the dictionary:
  - There will be an **Upload Dictionary** button or an **Edit Dictionary** area. The user (likely an admin or the template owner) can click this to either upload a YAML file from their computer or open a text editor modal to paste/write YAML.
  - If a dictionary is already associated with the template, the UI can display a snippet or status (e.g., "Dictionary: 5 services defined, last updated 2025-10-30 by admin").
  - On clicking Edit, a modal could show the YAML text which can be modified and saved.
  - We should integrate some basic validation feedback: e.g., if the YAML is not well-formed or missing major sections, the UI can show an error (the backend API will validate and return errors which UI should display). Possibly highlight line numbers if error can pinpoint (nice to have).
  - Because these dictionaries can be long, having a decent-size text area or a fullscreen editor with syntax highlighting would be beneficial for user experience, but at minimum a plain multi-line text area is okay. We might consider using a third-party YAML editor component if available in Primate’s tech stack.
  - **Versioning:** It might be useful to display the `version` field of the dictionary and maybe `vendor`/`product`. So the UI might parse the YAML client-side just to fetch those (or the API can parse and return structured info without secrets). For instance, show “Vendor: PaloAlto, Device OS 10.0, Dictionary Schema 1.0” if such info is present, to reassure the admin which dictionary is loaded.
  - Additionally, consider allowing **downloading** the current dictionary (for backup or editing in an external editor). A “Download Dictionary” link could simply fetch the raw YAML from the API.
- **Template Detail Summary:** Perhaps in the main template list or detail summary, indicate if a template has a dictionary and NICs configured. For example, a template of type VNF could show a label “VNF (dictionary attached)” to differentiate from ones that might not yet have one. This helps admins identify which templates are fully configured for use as a VNF.
Network Creation Wizard (VNF-backed Network)
When a user (admin or end-user with appropriate rights) creates a new isolated network, we provide an option to use a VNF as the router:
- **Selecting Network Type:** In the “Create Network” dialog, we can add a checkbox or a toggle for “Use a VNF Appliance as Network Router/Firewall”. If the user checks this:
  - A drop-down appears to **select a VNF Template**. This list will show available VNF templates (perhaps filtered to those the user has access to in that zone).
  - Possibly also select a service offering for the VNF VM if we want to allow sizing it (though we could default to a standard offering if not specified). If we want to keep it simple, possibly pick a default offering or one tied to the template.
  - If we know the network will require a specific network offering (that includes VNF provider), the UI can auto-select it or hide that complexity. For example, prior to this feature, you would select a network offering (like “Default Isolated Network with VR”). Now we might have a specialized offering “Isolated Network with VNF”. The UI could present a simplified view: choose VNF template and we internally map to the correct offering. Alternatively, we list the offerings and mark which are VNF-enabled and once selected, ask for a VNF template. The exact approach might depend on how we implement offerings. Ideally, to minimize user confusion, just asking for the VNF template might be enough and CloudStack picks an offering behind the scenes (or uses a global config for the offering to use for VNF networks).
  - **Dictionary Override:** Provide an optional field for dictionary override. This could be a file upload or text area (similar to template’s dictionary editor). If the user wants to override or customize the dictionary for this network, they can provide it here. If they leave it blank, the template’s built-in dictionary will be used. If provided, CloudStack will store this override (perhaps in the `network_details` or a new table).
  - We should caution in the UI that overriding is advanced. Possibly only show the override option to admins or if a certain “Advanced” toggle is clicked, to avoid confusing normal users.
- **Network Creation Flow:** The rest of the fields (network name, CIDR if required, etc.) are filled as usual. When the user submits:
  - The UI calls the appropriate API (e.g., `createNetwork` with `vnftemplateid` and dictionary if given). It will then likely poll for the result (since deploying the VNF may be async).
  - We should handle the async feedback: creating a VNF network might take a bit longer than a normal network because it has to deploy a VM. The UI might show the network in an “Implementing” or “Deploying VNF...” state.
  - Once done, the network appears in the list with maybe a special icon indicating it’s VNF-backed (just as an idea, maybe a small router icon or the vendor icon if we have one).
Network Detail and Operations (VNF Network Management)
When viewing the details of a network that is backed by a VNF, the UI should present relevant information and controls:
- **Status and Identification:** At the top of the network details, where it shows the network name, offering, etc., it should indicate that this network uses a VNF. For example: “Network: MyNet (VNF-Enabled)” and perhaps show which template or appliance. Possibly “Router: pfSense-appliance-VM (VM ID 1234)”. That might even be a link to the VM details of the VNF in the Instances section. This lets admins quickly jump to the VNF VM if needed (to check console, etc.).
- **VNF/Broker Status Panel:** We can have a section summarizing:
  - **Broker (VR) Status:** e.g., “Broker VM: VR-99 (Running)”. If the VR is down or having issues, that would show here (like if VR stopped, then obviously the network is broken in general).
  - **VNF Connectivity:** e.g., “VNF Connection: **Healthy** (last contact 2m ago)”. This could reflect a heartbeat or simply the last time we successfully executed a command. We might implement a lightweight heartbeat (maybe the VR or CloudStack polls the VNF periodically with a trivial command like get version). If not, at least update it each time a rule is applied or on last reconcile. If a recent attempt failed, mark it as unhealthy.
  - **Last Reconcile:** e.g., “Last Reconciliation: 2025-10-30 17:55:00, Drift Detected: No”. If drift was found: “Drift Detected: Yes (click for details)”.
  - Perhaps show how many rules CloudStack thinks are configured vs how many device has (like “Firewall rules: 5 (CloudStack) / 5 (Device)” if we gather that info easily).
  - This panel basically conveys if the VNF is in sync and the communication is working.
- **Controls/Actions:**
  - **Edit Dictionary (Network Override):** If the network has an override dictionary or if the user wants to apply one now, allow editing it similar to the template UI. For instance, an admin might realize they need to tweak a command for this particular network’s device. The UI could show either “Using template’s default dictionary” or “Custom dictionary in use”. An “Edit” button would open the YAML editor. Saving would call an API (maybe `updateNetwork` with new dictionary). Ideally, after updating, CloudStack might attempt to re-apply all rules (since the logic might have changed) or at least mark that a reconcile is needed. We should probably force a reconciliation after a dictionary change to ensure the device is configured according to the new instructions.
  - **Test VNF Connection:** A button “Test Connection” will trigger a simple API call (perhaps we use the existing `ping` command or a new `testVnf` call). This will make the management server attempt a basic operation like retrieving the device’s version or hostname via the broker. The UI can then display success or failure. This is mostly for admin troubleshooting (to differentiate between, say, a device that’s down vs a misconfigured dictionary vs a network issue).
  - **Reconcile Now:** A button to manually start reconciliation on this network. On click, call the reconcile API and perhaps show a progress or refresh the drift status after completion. If any issues are found and auto-resolved, we might show a brief summary (“2 rules were re-applied successfully.”). If not auto-resolved, inform what’s still off (“Device has 1 unknown rule not managed by CloudStack.” Possibly advise the admin to check or remove it manually).
  - **Other Actions:** Standard network actions like Restart Network, Acquire new IP, etc., should still be available. But we need to consider their behavior:
    - **Restart Network:** If an admin restarts a VNF-backed network (with or without cleanup), what does that mean? Typically restart would reboot the VR or re-deploy it. Here, VR is not critical for connectivity (except DHCP). We might consider whether restart should also reboot the VNF appliance or not. Possibly not by default, since that can be disruptive. But if “cleanup” is chosen, maybe it should wipe and reconfigure rules (which essentially CloudStack can do by re-sending all rules to the device). Actually, if an admin chooses “Restart network with cleanup”, perhaps CloudStack could do a full reconciliation (delete all rules from device and reapply from DB) to ensure a clean state. The UI doesn’t need to change, but the backend will have to handle it properly.
    - **Shutdown Network:** If a network is to be destroyed, CloudStack will destroy the VNF VM and VR. The UI flows remain the same as deleting any network.
    - **Add VM to Network:** Attaching instances to the network is unchanged, but note: when a new VM gets an IP via DHCP, it gets the VNF as gateway because of how we set up DHCP. The UI doesn’t reflect that explicitly, but it’s fine.
- **Firewall/Network Services UI:** The existing UI pages for Firewall rules, Port Forwarding, Load Balancing, etc., will continue to be used for VNF networks:
  - The user will go to the Firewall section of the network and click “Add Firewall Rule” as usual. They fill in CIDR, protocol, port, etc., same as before. When they submit, the UI calls the same API (`createFirewallRule`). CloudStack backend handles it with VNF. The UI can show it in the list of rules just like with VR (since CloudStack still creates a rule object in its DB).
  - One difference: with VR, applying a rule is usually near-instant. With a VNF, there might be a slight delay if the device is slow or the call takes a second or two. The API is async anyway, so UI should already handle rules being created asynchronously (most network APIs in CloudStack are async). Primate likely shows a pending state until it’s done. We should ensure any extended delay is communicated or at least doesn’t break the UX.
  - In some cases, a VNF might not support a certain service that the network offering nominally would allow. For example, if the VNF doesn’t do load balancing at all, but the offering includes LB, what do we do? Ideally, we would not allow the user to add an LB rule, and the UI should hide or disable the Load Balancer tab for that network. We can achieve this by marking which services are available on the network. CloudStack knows the provider (VNF) and could possibly know from the dictionary which services are defined. If a service (like LB) has no entry in the dictionary, CloudStack could consider it unsupported. We might propagate that info to the UI (perhaps via the API when fetching network details, include a list of supported services). Then the UI can avoid showing sections for unsupported features. This prevents user confusion and errors.
  - The experience of adding/editing rules thus remains consistent with normal networks, fulfilling the promise of a unified user experience across different underlying tech?26†L220-L228?. The user doesn’t have to know that behind the scenes an API call is going to Fortinet vs applying an iptables rule; they just see that the firewall rule was added and works.
- **VNF Appliance VM in UI:** The VNF appliance is a VM, and CloudStack actually shows VNFs in a separate “VNF Appliances” section (as per 4.19 UI) or possibly alongside instances. We should ensure that:
  - The VNF VM is visible to the user/account who owns the network (likely yes, since it’s deployed in their account as a special VM).
  - It might be marked or tagged as a VNF so that the instances list can filter or label it (for clarity, e.g., an icon or text “(VNF)” next to the VM name).
  - Users can open the console of the VNF or stop/start it (unless we restrict that). If a user attempts to stop their VNF VM, perhaps a warning “Stopping this VNF will disrupt network connectivity for MyNet” could be shown to deter them.
  - The template icon for VNF templates might be different to highlight it's a network function (in some UI screenshots, there’s mention of an ICON detail which might allow a vendor logo).
- **Feedback and Errors:** The UI should gracefully handle errors from the VNF operations:
  - If adding a firewall rule fails (say the VNF returned an error), the API call might return an error condition. The UI should display it like “Failed to add rule: <error message>”. The error might be generic or could include something from the device (e.g., “device reported: out of capacity”).
  - If the VNF is unreachable, the user might get an error when attempting any config: “Network router not responding” or similar. This is analogous to if a VR was down. We could refine the message to “VNF appliance not reachable” for clarity.
  - Any such error should also reflect in the network status possibly (so the user knows the network is in trouble).
- **UI Permissions:** Likely, regular end-users who can create their own isolated networks can also use VNF templates that they have access to (they might register their own VNF template or use one provided by admin). This should be fine. The UI just needs to ensure they see only templates they are allowed (just like they see their own templates normally). Admins can see all etc. Uploading dictionaries on templates might be restricted to template owner or admin – which is consistent with how template permissions work.
All these UI changes aim to make the introduction of VNFs as seamless as possible. A user deploying a VNF-backed network will follow almost the same flow as deploying a normal network. The difference is just choosing a VNF template instead of relying on the default VR. Once up, they manage firewall/NAT through the same screens. Meanwhile, administrators get additional tools (dictionary uploads, test connections, reconcile) to configure and maintain these integrations. The **consistent look and feel** ensures there’s no steep learning curve, while still providing the new functionality front-and-center for those who need it.
Maintainability, Extensibility, and Alignment with CloudStack Architecture
Finally, it’s crucial that the implementation of this VNF framework is done in a clean, maintainable way, respecting CloudStack’s modular architecture. We want to make sure this feature is easy to extend (for new vendors or future enhancements) and doesn’t become a brittle one-off. Key considerations include code structure, versioning, safety checks, and reusing CloudStack’s patterns.
- **Modular Code Structure:**  
  CloudStack’s networking is built around a plug-in architecture for network elements and service providers. We will follow this by creating a **VNF Provider plugin**. This likely lives under the `plugins` directory in the codebase (or could be core if tightly integrated, but better as a plugin module to keep optional aspects separate). The provider will implement interfaces for the services it supports. For example:
  - Implement `FirewallServiceProvider`, `PortForwardingServiceProvider`, `LoadBalancingServiceProvider`, etc., as applicable. These interfaces define methods like `applyFirewallRules(Network network, List<FirewallRule> rules, ... )`.
  - We can have one unified class (e.g., `VNFNetworkProvider`) implement all needed interfaces if we want a single provider handling multiple services. This is similar to how the VirtualRouter element handles firewall, NAT, LB all in one.
  - Alternatively, break it down by service (but likely one provider instance per network that covers all services via dictionary).
  - CloudStack’s dependency injection will wire this provider into the NetworkManager if the configuration says so (like when a network offering uses this provider).
  
  Internally, the plugin will use helper classes:
  - A **Dictionary Manager/Parser**: responsible for loading the YAML (from DB or file) and validating it. We might use SnakeYAML library to parse YAML into a Map or custom POJO structure. This component ensures the YAML matches the expected schema and can provide helpful errors if not (like “Missing services section”).
  - A **VNF Request Builder/Renderer**: for a given CloudStack operation (like a specific firewall rule), this takes the dictionary template and the CloudStack data and produces a concrete request object/string. This could involve simple find-and-replace for placeholders or a tiny templating mechanism. We should carefully handle types (string vs number) and quoting in JSON, etc. One approach is to allow the dictionary to be partly YAML/JSON so that we can use actual structures instead of raw string concatenation. For example, if the dictionary body is parsed as a map with placeholders as values, we can programmatically replace them and then dump to JSON. This avoids issues with quotes or formatting.
  - A **Broker Client**: abstract an interface like `VnfBrokerClient.sendRequest(networkId, VnfRequest request)`. There could be multiple implementations of this interface:
    - `VirtualRouterVnfClient` which finds the VR for the network and sends an agent command or HTTP request to it.
    - In the future, `DirectVnfClient` (if management can talk directly to VNF).
    - `ExternalControllerVnfClient` if integrating with an external orchestrator. Each would implement `sendRequest` accordingly. The VNF provider would get an instance of this based on configuration (likely always VR type initially).
  - We also need classes for the agent side if we go with agent commands. That includes a `VnfBrokerRequestCommand` carrying all needed fields, and a corresponding `VnfBrokerResponse`.
- **Version Control and Schema Evolution:**  
  We already plan for a `version` field in the dictionary. The code should perhaps register what version(s) it supports. For example, if in CloudStack 4.21 we define version 1.0, and in future 4.23 we have 2.0, the parser might branch based on that. We can maintain backward compatibility by continuing to support 1.0 dictionaries even if 2.0 offers new features. This means not making breaking changes without good reason. Instead, add new fields in a way that if they’re missing, we have a default behavior.
  - For instance, if we later add an `update` operation support, a 1.0 dictionary won’t have it – CloudStack should handle that gracefully (maybe by doing delete+create internally if update is invoked, or by refusing update with a clear message).
  - We might eventually formalize the dictionary schema (could even publish a JSON Schema or in docs so that vendors can write their own dictionaries easily).
  - It’s also wise to include a CloudStack version check. Perhaps store a minimum CloudStack version that the dictionary is intended for, especially if later we add things that older CloudStack code wouldn’t understand. But since we embed version, that might suffice.
- **Safety and Validation:**  
  We touched on dictionary validation – implementing that carefully is important. We should validate:
  - YAML is parseable.
  - Required keys exist for core functionality. If something critical is missing (e.g., no `Firewall.create` but we know this device is expected to do firewall), maybe warn or error.
  - Placeholders reference known values. We can define a set of placeholders CloudStack will provide for each context. For example, for firewall rules: `${sourceCidr}, ${protocol}, ${startPort}, ${endPort}, ${IcmpType}` etc. If a dictionary uses `${fooBar}`, which CloudStack doesn’t know, that likely is a mistake – we should catch and warn about such unknown placeholders. We can either decide to ignore them (which would leave an empty string, likely bad) or reject the dictionary as invalid. Rejecting is safer.
  - The `access` section’s references (like `usernameRef`) should correspond to actual template details or secrets available. Possibly at template creation time, we can enforce that if `usernameRef` is set, the template has a detail by that name. If not, maybe warn the user to add it. This might be hard to validate strictly, but we can hint it.
  - The code should also handle runtime errors robustly:
    - If a dictionary is missing something (say no `successCode` given, or no `idPath` after a create), we should proceed with reasonable defaults (assume 200 OK, assume no external ID to store unless specified).
    - If the VR returns an unexpected reply (malformed JSON, etc.), handle that without crashing – just mark the operation failed with an appropriate message.
    - In essence, lots of try-catch around external interactions, and converting exceptions into CloudStack Alerts or events for admins.
- **Logging and Debugging:**  
  Because this involves integration, debugging can be challenging. We should include debug-level logging that can be enabled to trace dictionary processing and API calls. For example, a log (at DEBUG level) might output:
  ```
  Rendering firewall rule (CloudStack ID=42) for VNF: using template 'allow port 80' -> POST /api/fw ... body {...}
  ```
  and when response comes:
  ```
  Received response code 500, error: "Invalid port" – marking rule as failed.
  ```
  Ensure no sensitive info in logs though. If needed, mask them (e.g., replace actual token with ***).
  Also, when VR executes a command, maybe log on VR (in /var/log somewhere) that “Executed request to VNF IP X, endpoint Y, result OK”. This helps if we need to troubleshoot at VR level.
- **Testing:**  
  We will write unit tests for the dictionary parser (feed it sample YAMLs, including edge cases like missing fields, wrong types, ensure it errors or defaults as expected). Also tests for the rendering (given a CloudStack dummy object, and a dictionary with placeholders, do we get correct output). For the broker, perhaps a mock VR client to verify that the management code properly calls it with right data. If possible, integration tests (Marvin tests) can deploy a fake VNF (maybe a VM running a simple http echo server as a stand-in) to test end-to-end, but that might be complex. At least, test the flows up to the point of sending, using simulated responses.
- **Aligning with Service Model:**  
  CloudStack’s design uses *Network Offerings* to define which providers supply which services for a network?19†46-L54?. For a VNF-backed network, we will have a network offering where:
  - DHCP, DNS, etc., are provided by the VirtualRouter (or perhaps the new VR in a special mode).
  - Firewall, PortForwarding, etc., are provided by “VNFProvider”.
  - Possibly if certain services are unsupported by VNF, we exclude them from the offering (so CloudStack won’t even try). For example, if the VNF doesn’t do load balancing, the offering used should not include LB service. If a user wants LB, they might deploy a different VNF or use an additional device.
  - The code for network orchestration will need to accommodate that both a VR and a VNF VM are deployed for one network. Normally, one network offering corresponds to one provider for each service, and CloudStack deploys those provider resources as needed. Deploying two different VMs (VR and another) for one network is a bit new (though in redundant VR case it’s two of same type). We will ensure the orchestration sequences are updated: e.g., after network creation, it should trigger both VirtualRouterElement (to create VR) and VnfProvider (to create VNF VM). We might coordinate so VR comes up first to handle DHCP when VNF boots, etc.
  - We should keep the responsibilities distinct: VR element code might, upon seeing the network is VNF type, adjust how it configures itself (like skipping firewall configuration, and using a special DHCP option for gateway). That means we may need to modify VR element or pass context to it. This is a critical integration point: perhaps the NetworkManager, when configuring the VR, knows a VNF will be the gateway and can program the VR accordingly (for example, do not set itself as default gateway, and perhaps push a static route or just let VMs use VNF’s IP).
  - The VirtualRouter VM’s own scripts (like `/etc/dnsmasq` config generator) may need a tweak to set option 3 (gateway) to the VNF’s IP instead of its own. We can achieve this by having the management server pass a flag or the actual gateway IP to the VR via the agent command when setting up DHCP. This is a detail to implement but conceptually straightforward (many DHCP servers allow specifying an alternate gateway).
- **Extensibility for New Vendors:**  
  Once this framework is in place, adding a new vendor should ideally require:
  - Creating a VNF template (which might just be the VM image of the vendor’s appliance).
  - Writing a YAML dictionary for it.
  - Uploading that via UI.
  Now the user can deploy networks with that template and CloudStack will manage it. No new Java code. This is a huge win for flexibility. We should test it by perhaps creating a dictionary for a known open-source router (like VyOS or pfSense’s API if any) and see that we can drive it.
  If some vendor has a very custom need (say a multi-step commit process: JunOS style commit-confirm, etc.), the dictionary might not suffice. In such cases, either we extend dictionary capabilities (like allow specifying multiple calls in sequence for one operation), or those might require a custom Java plugin (which could still implement the provider interface directly). But given our target is to cover common scenarios, dictionary should handle 90% of use cases.
- **Maintenance:**  
  The code should be documented and follow CloudStack conventions (license headers, etc.). Future maintainers should easily identify the VNF plugin code. We can include a README in the plugin module describing how to add a new dictionary or what the structure is – useful for developers or advanced users.
  Also, because the VR script changes and the new communication port are non-trivial, we must ensure these are properly integrated into the CloudStack system VM build process. Updating the VR template might be needed. We should bump the VR version so that when someone upgrades CloudStack, they know to upgrade VRs to get VNF support.
- **Safety Checks on Upgrades:**  
  If CloudStack is upgraded and the dictionary schema changes (say from 1.0 to 1.1), we should ensure backward compatibility or provide a migration. It might be as simple as continuing to interpret old dictionaries (with a warning if deprecated fields are used). Possibly writing an upgrade script to adjust stored YAML if needed, but ideally avoid that by supporting old format in code.
By following the above practices, the implementation will remain **clean, modular, and aligned** with CloudStack’s design philosophy. We keep the core CloudStack code as untouched as possible, adding new components through extension points (providers, plugin interfaces)?26†L211-L218?. We also isolate vendor-specific differences to data (the YAML) rather than code, which drastically reduces maintenance burden and risk of bugs when supporting multiple vendors. The result will be a robust framework that not only addresses the immediate needs (firewall/NAT via VNF in 4.21.7) but also provides a foundation for future features like service chaining, multi-VNF scenarios, and integration with NFV orchestration systems, all while maintaining CloudStack’s hallmark of multi-tenancy, security, and user-friendly cloud management.

